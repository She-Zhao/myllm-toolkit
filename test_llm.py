"""
APIè°ƒç”¨ç¤ºä¾‹
æä¾›ä¸€ä¸ªåˆ©ç”¨APIè¿›è¡Œå¤šè½®å¯¹è¯çš„ç®€å•ç¤ºä¾‹    
"""
import os
from openai import OpenAI
from model_config import ModelConfigManager

# # åœ¨ä»£ç å¼€å¤´è®¾ç½®ä»£ç†çŽ¯å¢ƒå˜é‡
# os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7897'
# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7897'
# os.environ['ALL_PROXY'] = 'http://127.0.0.1:7897'

def initialize_client(api_key, base_url):
    if not api_key:
        raise ValueError("api_keyä¸ºç©º, è¯·æ£€æŸ¥çŽ¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®!")
    
    return OpenAI(
        api_key=api_key,
        base_url=base_url
    )

def chat_single(config_manager: ModelConfigManager, provider: str, model: str):
    model_config = config_manager.get_model_config(provider, model)
    client = initialize_client(api_key=model_config['api_key'], base_url=model_config['base_url'])
    
    system_prompt = "You are a helpful assistant, please add '>_<' after answering each question."
    user_message = "Hello!"
    conversation = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]

    print(f"ä½¿ç”¨æ¨¡åž‹: {model_config['provider']} - {model_config['model']}")
    print(f"æ¨¡åž‹æè¿°: {model_config['description']}")

    response = client.chat.completions.create(
        model = model_config['model'],
        messages = conversation,
        stream = False
    )
    
    print(f"LLMðŸ¤–: {response.choices[0].message.content}")
    # å¯¹äºŽæ€è€ƒæ¨¡åž‹ï¼Œå¯ä»¥é€šè¿‡reasoning_contentè®¿é—®æ€ç»´é“¾
    # print(f"LLMðŸ¤–: {response.choices[0].message.reasoning_content}")

def chat_multi(config_manager: ModelConfigManager, provider: str, model: str):
    model_config = config_manager.get_model_config(provider, model)
    client = initialize_client(api_key=model_config['api_key'], base_url=model_config['base_url'])
    system_prompt = "You are a helpful assistant, please add '>_<' after answering each question."
    conversation = [
        {"role": "system", "content": system_prompt}
    ] 
    
    print(f"ä½¿ç”¨æ¨¡åž‹: {model_config['provider']} - {model_config['model']}")
    print(f"æ¨¡åž‹æè¿°: {model_config['description']}")
    print("å¼€å§‹å¤šè½®å¯¹è¯ï¼Œè¾“å…¥ 'q' é€€å‡º\n")
    
    while True:
        user_input = input('humanðŸ‘¤:').strip()
        if user_input == 'q':
            print('å¯¹è¯ç»“æŸï¼')
            break
        
        if not user_input:
            print('ç”¨æˆ·è¾“å…¥ä¸èƒ½ä¸ºç©º!')
            continue
        
        conversation.append({"role": "user", "content": user_input})
        response = client.chat.completions.create(
            model = model_config['model'],
            messages = conversation,
            stream = False
        )
        
        ai_response = response.choices[0].message.content
        conversation.append({"role": "assistant", "content": ai_response})
        print(f"LLMðŸ¤–: {ai_response}")

if __name__ == "__main__":
    config_manager = ModelConfigManager()
    provider = 'openai'
    model = 'gpt-5'

    # chat_single(config_manager, provider, model)       # å•è½®å¯¹è¯æµ‹è¯•
    chat_multi(config_manager, provider, model)      # å¤šè½®å¯¹è¯æµ‹è¯•
    