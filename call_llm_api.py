"""
è°ƒç”¨åœ¨çº¿LLM APIå¤„ç†å¤šæ¨¡æ€æ•°æ®

è¯¥æ¨¡å—è¯»å–JSONLæ ¼å¼çš„è¾“å…¥æ–‡ä»¶ï¼Œè°ƒç”¨LLM APIå¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰ï¼Œ
å¹¶å°†ç»“æœå†™å…¥è¾“å‡ºæ–‡ä»¶ã€‚

author:zhaoshe
"""

import os
import json
import base64
import asyncio
from tqdm import tqdm
import argparse
from tqdm.asyncio import tqdm_asyncio
from typing import List, Dict, Any
from openai import AsyncOpenAI, APIError
# from openai import OpenAI
from model_config import ModelConfigManager  # å‡è®¾ä½ æœ‰è¿™ä¸ªé…ç½®æ–‡ä»¶

os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7897'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7897'
os.environ['ALL_PROXY'] = 'http://127.0.0.1:7897'

def initialize_client(api_key: str, base_url: str) -> AsyncOpenAI:
    if not api_key:
        raise ValueError("API KEYä¸ºç©º!")
    
    return AsyncOpenAI(
        api_key = api_key,
        base_url = base_url,
    )

def encode_image_to_base64(image_path: str) -> str:
    """è¯»å–å›¾åƒæ–‡ä»¶å¹¶ç¼–ç ä¸ºbase64æ ¼å¼

    Args:
        image_path (str): å›¾åƒæ–‡ä»¶çš„è·¯å¾„

    Returns:
        str: "data:jpeg;base64,{base64_string}æ ¼å¼çš„å­—ç¬¦ä¸²
    """
    with open(image_path, 'rb') as image_file:
        binary_data = image_file.read()
        base64_string = base64.b64encode(binary_data).decode('utf-8')
        
        mime_type = 'image/jpeg'
        if image_path.lower().endswith('.png'):
            mime_type = 'image/png'
        elif image_path.lower().endswith('.bmp'):
            mime_type = 'image/bmp'
        elif image_path.lower().endswith('.webp'):
            mime_type = 'image/webp'            
            
    return f'data:{mime_type};base64,{base64_string}'

def build_send_message(sample: Dict[str, Any]) -> List[Dict[str, Any]]:
    """æ ¹æ®sft.jsonlä¸­çš„æ¯æ¡jsonæ•°æ®ï¼Œæ„é€ è¾“å…¥ç»™apiçš„æ•°æ®

    Args:
        sample (Dict[str, Any]): å•ä¸ªjsonæ•°æ®

    Returns:
        List[Dict[str, Any]]: è¾“å…¥ç»™æ¨¡å‹çš„æ•°æ®
    """
    image_paths = sample['image']
    human_prompt_raw = sample['conversation'][0]['value']
    human_prompt_list = human_prompt_raw.split('<image>')
    
    content = []
    for idx, human_prompt in enumerate(human_prompt_list):
        if human_prompt != '':
            content.append({
                'type': 'text',
                'text': human_prompt
            })
        
        if idx < len(image_paths):
            try:
                image_path = image_paths[idx]
                base64_image = encode_image_to_base64(image_path)
                content.append({
                    'type': 'image_url',
                    'image_url': {'url': base64_image}
                })
                
            except Exception as e:
                print(f"é‡åˆ°é”™è¯¯ {e}, æ— æ³•ç¼–ç å›¾åƒ {image_path}, å·²è·³è¿‡.")
                continue
    
    message = [{
        'role': 'user',
        'content': content
    }]
    
    return message


async def process_single_task(
    client: AsyncOpenAI,
    sample: Dict[str, Any],
    model: str,
    semaphore: asyncio.Semaphore
) -> Dict[str, Any]:
    """æäº¤å•ä¸ªAPIè°ƒç”¨çš„åç¨‹å‡½æ•°, å½“è°ƒç”¨å¤±è´¥çš„æ—¶å€™ä¼šè‡ªåŠ¨ä¿å­˜'ERROR',
    å¯ä»¥å¯¹sample['conversation'][1]['value']è¿›è¡Œåˆ¤æ–­æ¥å‰”é™¤è°ƒç”¨å¤±è´¥çš„æ•°æ®

    Args:
        client (AsyncOpenAI): OpenAIå®¢æˆ·ç«¯ï¼ˆæ”¯æŒå¼‚æ­¥ï¼‰
        sample (Dict[str, Any]): å¾…å¤„ç†çš„å•ä¸ªæ ·æœ¬ï¼ˆä»»åŠ¡ï¼‰
        model (str): è°ƒç”¨APIçš„åç§°ï¼ˆè°ƒç”¨æ¨¡å‹çš„åç§°ï¼‰
        semaphore(asyncio.Semaphore): æ¥æ”¶ä¿¡å·é‡

    Returns:
        Dict[str, Any]:åŒ…å«æ¨¡å‹å›å¤çš„æ•°æ®
    """
    async with semaphore:       # ç¡®ä¿åœ¨ä»»ä½•æ—¶å€™ï¼Œæœ€å¤šéƒ½åªæœ‰semaphoreä¸ªä»»åŠ¡åŒæ—¶æ‰§è¡Œwithå†…çš„ä»£ç 
        try:
            messages = build_send_message(sample)
            response = await client.chat.completions.create(
                model = model,
                messages = messages
                # max_tokens=10
            )
            
            # æ£€æŸ¥ response å’Œ choices æ˜¯å¦æœ‰æ•ˆ
            if response and response.choices and len(response.choices) > 0:
                # æ£€æŸ¥ message å’Œ content æ˜¯å¦æœ‰æ•ˆ
                if response.choices[0].message and response.choices[0].message.content:
                    ai_response = response.choices[0].message.content
                    sample['conversation'][1]['value'] = ai_response
                else:
                    # API æˆåŠŸäº†ï¼Œä½† message.content ä¸ºç©º
                    print(f"âŒ API è­¦å‘Š (ID: {sample['id']}): å“åº”ä¸­ç¼ºå°‘ message.contentã€‚")
                    sample['conversation'][1]['value'] = 'ERROR: Empty message content'
            else:
                # API æˆåŠŸäº†ï¼Œä½†è¿”å›äº†ç©ºçš„ 'choices' åˆ—è¡¨æˆ– None
                print(f"âŒ API è­¦å‘Š (ID: {sample['id']}): å“åº”ä¸­ç¼ºå°‘ 'choices'ã€‚")
                sample['conversation'][1]['value'] = 'ERROR: Empty choices list'
            # --- ğŸš€ å¥å£®æ€§ä¿®å¤ç»“æŸ ---
        
        except APIError as e: # æ›´å…·ä½“åœ°æ•è· API é”™è¯¯
            print(f"âŒ API é”™è¯¯ (ID: {sample['id']}): {e} ")
            sample['conversation'][1]['value'] = f'ERROR: APIError {e}'
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯ (ID: {sample['id']}): {e} ")
            sample['conversation'][1]['value'] = f'ERROR: Exception {e}'
        
    return sample
    
    
async def process_batch_task(args):
    """
    å¼‚æ­¥æ‰¹å¤„ç†çš„ä¸»åè°ƒå‡½æ•°ã€‚

    è¯¥å‡½æ•°è´Ÿè´£ï¼š
    1. åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚
    2. è¯»å–å·²å®Œæˆçš„ä»»åŠ¡IDï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰ã€‚
    3. è¯»å–å¹¶è¿‡æ»¤å¾…å¤„ç†çš„ä»»åŠ¡ã€‚
    4. åˆ›å»ºä¿¡å·é‡ä»¥æ§åˆ¶å¹¶å‘ã€‚
    5. ä½¿ç”¨ `asyncio.as_completed` å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ã€‚
    6. åœ¨ä»»åŠ¡å®Œæˆæ—¶ç«‹å³å°†å…¶ç»“æœè¿½åŠ å†™å…¥è¾“å‡ºæ–‡ä»¶ã€‚

    Args:
        args (argparse.Namespace): 
            ä»å‘½ä»¤è¡Œè§£æçš„å‚æ•°, å¿…é¡»åŒ…å«:
            - provider (str): API æä¾›å•†
            - model (str): æ¨¡å‹åç§°
            - input_file (str): è¾“å…¥çš„ .jsonl ä»»åŠ¡æ–‡ä»¶
            - output_file (str): è¾“å‡ºçš„ .jsonl ç»“æœæ–‡ä»¶
            - concurrency (int): æœ€å¤§å¹¶å‘æ•°
    """
    print(f"ğŸš€ å¼€å§‹è°ƒç”¨APIï¼ˆå¼‚æ­¥ï¼‰...")
    print(f"    å¹¶å‘æ•°é‡: {args.concurrency}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    config_manager = ModelConfigManager()
    model_config = config_manager.get_model_config(args.provider, args.model)
    client = initialize_client(api_key=model_config['api_key'], base_url=model_config['base_url'])
    print(f"    æ¨¡å‹: {args.provider} - {args.model}")

    # è¯»å–å·²å®Œæˆçš„ä»»åŠ¡
    completed_ids = set()
    
    if os.path.exists(args.output_file):
        with open(args.output_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    line_data = json.loads(line)
                    completed_ids.add(line_data['id'])
                except Exception as e:
                    pass
    print(f"å·²åŠ è½½ {len(completed_ids)} ä¸ªå·²å®Œæˆçš„ä»»åŠ¡")
    
    # è¯»å–æ‰€æœ‰å¾…è°ƒç”¨apiçš„æ•°æ®
    task_to_process = []
    with open(args.input_file, 'r', encoding = 'utf-8') as f_in:
        for line in f_in:
            task = json.loads(line)
            if task['id'] not in completed_ids:
                task_to_process.append(task)

    total_tasks = len(task_to_process)
    if total_tasks == 0:
        print(f"âœ”ï¸æ‰€æœ‰ä»»åŠ¡å‡å·²å®Œæˆï¼Œæ— éœ€å¤„ç†!")
    else:
        print(f"âš¡å…±æ‰¾åˆ° {total_tasks} ä¸ªå¾…å¤„ç†ä»»åŠ¡")
    
    # åˆ›å»ºä¿¡å·é‡
    semaphore = asyncio.Semaphore(args.concurrency)
    
    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡çš„åç¨‹ï¼Œå­˜å‚¨åœ¨listä¸­
    coroutines = [process_single_task(client, sample, model_config['model'], semaphore) for sample in task_to_process]
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡å¹¶ä¿å­˜ç»“æœ
    print(f"âœ¨âœ¨å¼€å§‹å¹¶å‘æ‰§è¡Œ {len(coroutines)} ä¸ªä»»åŠ¡ï¼Œæœ€å¤§å¹¶å‘æ•°: {args.concurrency}")
    results_count = 0
    try:
        with open(args.output_file, 'a', encoding='utf-8') as f_out:
            for future in tqdm(asyncio.as_completed(coroutines), total=total_tasks, desc="Processing tasks"):
                result = await future           # awaitè·å–äº‹ä»¶å¾ªç¯ä¸­çš„ä¸€ä¸ªå¤„ç†ç»“æœ
            
                if result:
                    f_out.write(json.dumps(result, ensure_ascii=False) + '\n')
                    f_out.flush()           # ç«‹åˆ»å°†æ–‡ä»¶å†™å…¥
                    results_count += 1
    except Exception as e:
        print(f"âŒ  å¾ªç¯å¤„ç†è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯: {e}")
        print(f"    å·²å¤„ç†  {results_count} / {total_tasks} ä¸ªä»»åŠ¡")
        return
    
    print(f"\nâœ… ä»»åŠ¡å¤„ç†å®Œæˆï¼Œ{results_count} ä¸ªæ–°ç»“æœå·²è¿½åŠ è‡³ {args.output_file}")
    await client.close()

def main():
    """
    ä¸»å…¥å£å‡½æ•°ï¼šè§£æå‘½ä»¤è¡Œå‚æ•°ã€‚
    """
    parser = argparse.ArgumentParser(description="æ‰¹é‡è°ƒç”¨LLM API, å¼‚æ­¥æ§åˆ¶, æ”¯æŒæ–­ç‚¹ç»­è·‘")
    parser.add_argument('--provider', type=str, required=True, help='APIæä¾›å•†')
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹åç§°')
    parser.add_argument('--input_file', type=str, required=True, help='è¾“å…¥çš„ .jsonl å¾…å¤„ç†æ–‡ä»¶')
    parser.add_argument('--output_file', type=str, required=True, help='è¾“å‡ºçš„å¤„ç†ç»“æœæ–‡ä»¶')
    parser.add_argument('--concurrency', type=int, default=10, help='å¹¶å‘è°ƒç”¨æ•°é‡, é»˜è®¤ä¸º10')

    args = parser.parse_args()
    asyncio.run(process_batch_task(args))

if __name__ == "__main__":
    # main()
    
    test_args = argparse.Namespace()
    
    # 3. åœ¨è¿™é‡Œç¡¬ç¼–ç æ‚¨çš„è°ƒè¯•å‚æ•°
    # -----------------------------------------------
    test_args.provider = 'qwen'
    test_args.model = 'qwen-vl-plus' # æ›¿æ¢ä¸ºæ‚¨çš„æµ‹è¯•æ¨¡å‹
    test_args.input_file = './example/sft_dataset_cot.jsonl' # æ›¿æ¢ä¸ºæ‚¨çš„æµ‹è¯•è¾“å…¥æ–‡ä»¶
    test_args.output_file = './example/sft_dataset_cot_result.jsonl' # æ›¿æ¢ä¸ºæ‚¨çš„æµ‹è¯•è¾“å‡ºæ–‡ä»¶
    test_args.concurrency = 10    # è°ƒè¯•æ—¶ä½¿ç”¨è¾ƒä½çš„å¹¶å‘
    # -----------------------------------------------
    
    print(f"--- æ­£åœ¨ä»è„šæœ¬ä¸­å¯åŠ¨ call_llm_api_robust (è°ƒè¯•æ¨¡å¼) ---")
    print(f"   Provider: {test_args.provider}")
    print(f"   Model: {test_args.model}")
    print(f"   Input: {test_args.input_file}")
    print(f"   Output: {test_args.output_file}")
    print(f"   Concurrency: {test_args.concurrency}")
    
    # 4. è¿è¡Œä¸»åç¨‹
    # asyncio.run() ä¼šè‡ªåŠ¨åˆ›å»ºå’Œç®¡ç†äº‹ä»¶å¾ªç¯
    try:
        asyncio.run(process_batch_task(test_args))
        print(f"--- è„šæœ¬è°ƒç”¨æ‰§è¡Œå®Œæ¯• ---")
    except Exception as e:
        print(f"--- è„šæœ¬è°ƒç”¨æ—¶å‘ç”Ÿé”™è¯¯: {e} ---")
